# Part 4: Draft of Technical Report

My goal for this project is to use Natural Language Processing and K-Means clustering to create clusters of Jeopardy questions. These clusters could be used as a type of study guide for potential Jeopardy contestants, enabling them to see what topics appear most frequently on the show, as well as study sample questions from each topic.

My data consist of almost 217,000 Jeopardy questions and answers. For each question, I have the show number, the air date, the round the question was asked in, the category, the value of the question, the text of the question, and the answer to the question. My data was obtained from a .csv I found in a [Reddit post](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/). The .csv was created by scraping the [J! Archive](http://www.j-archive.com/), a fan created and maintained archive of Jeopardy episodes dating back to 1984. The dataset contains most of the questions from 1984 to early 2012. My data was already in a clean, easily usable format, so I didn't have to do too much cleaning. I did filter out a few of audio/video questions so that my dataset only contained text questions.

Since my goal is to cluster Jeopardy questions, the only columns of my dataset I used were the Question (and possibly the Answer). Including the Category column tended to wildly affect the quality of the clusters, so it was not included in the model. I did use the Category to analyze the results of the clustering. I also may use the Air Date column in conjunction with the clusters generated by my model to do some analysis of the different seasons of Jeopardy and how the content of the show has changed over time.

The obvious choice for my modeling technique is [K-Means Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Prior to performing the clustering, I did some NLP pre-processing on the text data to generate features. I tokenized the question text, and used [NLTK's Snowball Stemmer](http://www.nltk.org/_modules/nltk/stem/snowball.html) to stem the words. I've also experimented with using only the nouns in the text of the question, but haven't yet decided which will be the final version of my model. After performing the NLP pre-processing on the raw text, I used [sklearn's TF-IDF Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to transform the text data into a numerical sparse matrix which I could then input into my clustering model.

To determine the number of clusters, I compared the [silhouette scores](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) of several models. In order to reduce the amount of time needed to compare several models, I used only 10% of the dataset for most of the preliminary testing and parameter selection. 

In order to assign a topic name to each of the clusters, I am attempting to use topic modeling and [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). My work is still in preliminary stages, so at the moment I'm unsure how successful this will be.